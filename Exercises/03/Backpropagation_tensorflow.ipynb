{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks in TensorFlow\n",
    "\n",
    "\n",
    "### Goals: \n",
    "- Introduce the basics of `TensorFlow`\n",
    "- Auto-differentiation\n",
    "- Implement the digit classifier using the low level `TensorFlow` API without Keras abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow\n",
    "\n",
    "TensorFlow is a dynamic graph computation engine, that allows automatic differentiation of each node. Tensorflow is the default computational backend of the Keras library. I can also be used directly from Python to build deep learning models.\n",
    "\n",
    "- https://www.tensorflow.org \n",
    "- https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "\n",
    "TensorFlow builds where nodes may be:\n",
    "- **constant:** constants tensors, such as training data;\n",
    "- **Variables:** any tensor that is meant to be updated when training, such as parameters of the models.\n",
    "\n",
    "**Note** that we are using for this course the new version Tensorflow 2.0. This version cleaned the old cluttered api and uses by default dynamic graph of operations to make it natural to design a model interactively in Jupyter. Previously you defined the graph statically once, and then needed to evaluate it by feeding it some data. Now it is dynamically defined when executing imperative Python instructions which means that you can `print` any tensor at any moment or even use `pdb.set_trace()` to inspect intermediary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.Variable(0)\n",
    "b = tf.constant(2)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[0, 1], [2, 3]], dtype=tf.float32)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tf.Tensor can be converted to numpy the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable([1, 2], dtype=tf.float32)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(b, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(A, tf.reshape(b, (-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a function that computes the squared Euclidean norm of an 1D tensorf input x:\n",
    "\n",
    "- Use element wise arithmetic operations (`+`, `-`, `*`,  `/`, `**`)\n",
    "- Use `tf.reduce_sum` to compute the sum of the element of a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1, -4], dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_norm(x):\n",
    "    # TODO: sum of the squared elements of x\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/tf_squared_norm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_norm(x).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1, -4], dtype=tf.float32)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    result = squared_norm(x)\n",
    "\n",
    "\n",
    "variables = [x]\n",
    "gradients = tape.gradient(result, variables)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x = gradients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a gradient step to modify x in place by taking one step of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign_sub(0.1 * grad_x)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following gradient descent step many times consecutively to watch the decrease of the objective function and the values of `x` converging to the minimum of the `squared_norm` function.\n",
    "\n",
    "Hit `[ctrl]-[enter]` several times to execute the same Jupyter notebook cell over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    objective = squared_norm(x)\n",
    "    \n",
    "x.assign_sub(0.1 * tape.gradient(objective, [x])[0])\n",
    "\n",
    "print(f\"objective = {objective.numpy():e}\")\n",
    "print(f\"x = {x.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device-aware Memory Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitely place tensors on a device, use context managers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU:0\"):\n",
    "    x_cpu = tf.constant(3)\n",
    "    \n",
    "# with tf.device(\"GPU:0\"):\n",
    "#     x_gpu = tf.constant(3)\n",
    "x_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Digits Classifier in TensorFlow\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides dataset abstraction which makes it is to iterate over the data batch by batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(x, y, batch_size=128):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_dataset(X_train, y_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(dataset))\n",
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model using TensorFlow\n",
    "\n",
    "- Using TensorFlow, build a similar model (one hidden layer) as you previously did;\n",
    "- The input will be a batch coming from X_train, and the output will be a batch of ints;\n",
    "- The output do not need be normalized as probabilities, the softmax will be moved to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to use to test your randomly initialized model:\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random.normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == y)\n",
    "\n",
    "\n",
    "def test_model(model, x, y):\n",
    "    dataset = gen_dataset(x, y)\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch_x, batch_y in dataset:\n",
    "        preds.append(model(batch_x).numpy())\n",
    "        targets.append(batch_y.numpy())\n",
    "     \n",
    "    preds, targets = np.concatenate(preds), np.concatenate(targets)\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model there, and then execute the following cell to train your model.\n",
    "Don't hesitate to tweak the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "batch_size = 32\n",
    "hid_size = 15\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "# build the model and weights\n",
    "class MyModel:\n",
    "    def __init__(self, input_size, hid_size, output_size):\n",
    "        # TODO: randomly initialize all the internal variables of the model:\n",
    "        self.W_h = None # TODO\n",
    "        self.b_h = None # TODO\n",
    "        self.W_o = None # TODO\n",
    "        self.b_o = None # TODO\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # TODO: this method should implement the forward pass with\n",
    "        # tensorflow operations: compute the outputs, that is the\n",
    "        # unnormalized predictions of the network for a give batch\n",
    "        # of inputs vectors.\n",
    "        # No need to implement the softmax operations as we will\n",
    "        # move it the loss function instead.\n",
    "        \n",
    "        # Hint: you can use tf.matmul, tf.tanh, tf.sigmoid,\n",
    "        # arithmetic operations and so on.\n",
    "        return None\n",
    "    \n",
    "model = MyModel(input_size, hid_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/tf_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implements a training loop in Python. Note the use of `tf.GradientTape` to automatically compute the gradients of the loss w.r.t. the different parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for e in range(num_epochs):\n",
    "    train_dataset = gen_dataset(X_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    for batch_x, batch_y in train_dataset:\n",
    "        # tf.GradientTape records the activation to compute the gradients:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(batch_x)\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits))\n",
    "            losses.append(loss.numpy())\n",
    "            \n",
    "        # Here we ask for the gradients of dL/dW_h, etc.\n",
    "        dW_h, db_h, dW_o, db_o = tape.gradient(\n",
    "            loss, [model.W_h, model.b_h, model.W_o, model.b_o])\n",
    "        \n",
    "        # Update the weights as a Stochastic Gradient Descent would do:\n",
    "        model.W_h.assign_sub(learning_rate * dW_h)\n",
    "        model.b_h.assign_sub(learning_rate * db_h)\n",
    "        model.W_o.assign_sub(learning_rate * dW_o)\n",
    "        model.b_o.assign_sub(learning_rate * db_o)\n",
    "        \n",
    "    train_acc = test_model(model, X_train, y_train)\n",
    "    test_acc = test_model(model, X_test, y_test)\n",
    "    print(\"Epoch {}, train_acc = {}, test_acc = {}\".format(e, round(train_acc, 4), round(test_acc, 4)))\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (Bonus)\n",
    "\n",
    "- add L2 regularization with $\\lambda = 10^{-4}$\n",
    "- train with arbitrary number of layers: only pass the layer sizes as hyperparameter to the model class constructor (`__init__` method)\n",
    "- try implementing momentum\n",
    "- you may use tensorboard (https://www.tensorflow.org/how_tos/summaries_and_tensorboard/) to monitor loss and display graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
