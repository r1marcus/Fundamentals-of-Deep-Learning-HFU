{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet design\n",
    "\n",
    "In this notebook we will design our own ConvNet and see some existing applications.\n",
    "\n",
    "We will also see the three different methods to define a Keras model:\n",
    "\n",
    "- Sequential\n",
    "- Functional\n",
    "- Object-Oriented\n",
    "\n",
    "The goal of this notebook is not to compare models performance, as we are limited on compute power, but to compare model architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "h, w = x_train.shape[1:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], h, w, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], h, w, 1)\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=10000, random_state=42)\n",
    "\n",
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(-1))\n",
    "plt.title(y_train[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"{} unique labels.\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeNet\n",
    "\n",
    "Let's define a (slightly modified) LeNet model introduced by Yann Le Cun in 1998 ([paper url](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). The model is very simple and can be defined with the **Sequential** API.\n",
    "\n",
    "![lenet archi](images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "lenet = Sequential(name=\"LeNet-5\")\n",
    "\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation=\"tanh\", padding=\"same\", input_shape=input_shape, name=\"C1\"))\n",
    "lenet.add(MaxPool2D(pool_size=(2, 2), name=\"S2\"))\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name=\"C3\"))\n",
    "lenet.add(AvgPool2D(pool_size=(2, 2), name=\"S4\"))\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name=\"C5\"))\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84, activation='tanh', name=\"F6\"))\n",
    "lenet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "lenet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while LeNet was first defined using either `tanh` or `sigmoid`, those activations are now rarely used. As seen in Lab02, both activations saturate on very small and large values making their gradient almost null.\n",
    "\n",
    "Now most network use `ReLU` as hidden activation function or one of its derivative (https://keras.io/layers/advanced-activations/).\n",
    "\n",
    "## 2. Inception\n",
    "\n",
    "Inception models were introduced in 2014 by Szegedy et al. ([paper url](https://arxiv.org/abs/1409.4842)).\n",
    "\n",
    "Convolutions have an effective receptive field: the bigger the kernels, and the deeper the model, a features pixel will *see* more image pixels. Read this for a good explanation: [medium blog](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
    "\n",
    "In Inception, convolution kernels of different sizes are combined. Small kernels see small clusters of features (think a detail as an eye) while big kernels see big clusters of features (think a face).\n",
    "\n",
    "![inception archi](images/inception.png)\n",
    "\n",
    "This time, use the **Functional** API to define a single Inception layer like the previous image\n",
    "Exemple usage:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "```\n",
    "\n",
    "The layer is first instancied (first pair of parenthesis) then called on a tensor (second set of parenthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    # TODO: define the 4 branches\n",
    "    branch1x1 = None\n",
    "    branch5x5 = None\n",
    "    branch3x3 = None\n",
    "    branch_pool = None\n",
    "\n",
    "    # TODO: merge it using Concatenate layer, use Concatenate? for more info\n",
    "    output = None\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mini_inception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet\n",
    "\n",
    "ResNet (*Residual Networks*) models were introduced by He et al. in 2015 ([paper url](https://arxiv.org/abs/1512.03385)). They found that more layers improved the performance but unfortunatly it was hard to backpropagate the gradients up to first layers.\n",
    "\n",
    "A trick to let the gradients \"*flow*\" easily is to use shortcut connection that let the forward tensor untouched (aka a *residual*):\n",
    "\n",
    "![resnet archi](images/resnet.png)\n",
    "\n",
    "This time, code a ResNet layer using the **Oriented-Object** API:\n",
    "\n",
    "Exemple usage:\n",
    "```python\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Add\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "        \n",
    "        # TODO: define needed layers, use Add to combine the shortcut with the convs' output.\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 42\n",
    "    \n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 1337\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mini_resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "\n",
    "Batch Normalization is not an architecture but a layer. Introduced by Ioffe et al. in 2015 ([paper url](https://arxiv.org/abs/1502.03167)). Here is an extract from their abstract:\n",
    "\n",
    "> Training Deep Neural Networks is complicated by the fact that the **distribution of each layerâ€™s inputs changes during training, as the parameters of the previous layers change**. This slows down the training by requiring lower learningrates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.  We refer to this phenomenon as **internal covariate shift**, and address the problem by **normalizing layer inputs**.\n",
    "\n",
    "The results are that ConvNet trained with BatchNorm converge faster and with better results. Nowadays all (or almost all) networks are use it or one of its variants. See this [article on normalization](https://arthurdouillard.com/post/normalization/) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic block is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Layer):\n",
    "    def __init__(n_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = Conv2D(n_filters, kernel_size=kernel_size, use_bias=False)\n",
    "        self.bn = BatchNormalization(axis=3)\n",
    "        self.activation = Activation(\"relu\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activation(\n",
    "            self.bn(self.conv(inputs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That you can place multiple times into your network as Lego blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Separable Convolutions\n",
    "\n",
    "ConvNet usually have a lot of parameters because of their large depth. A trick to trim the number of parameters with minimal performance loss is to use **separable convolution**.\n",
    "\n",
    "The standard convolution has quite a lot of parameters (but still much less than a Fully Connected layer!):\n",
    "\n",
    "![conv](images/conv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(name=\"Conv Model\")\n",
    "conv_model.add(Conv2D(8, kernel_size=(3, 3), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: \n",
    "\n",
    "- How many parameters does this convolution has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_model.build((None, *input_shape))\n",
    "# conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separable convolutions are made of two convolutions:\n",
    "\n",
    "- A **depthwise convolution**, a single kernel is created per input channels, spatial information is affected, but channels information is not shared\n",
    "\n",
    "![depthwise conv](images/depthwise.png)\n",
    "\n",
    "- A **pointwise convolution**, is a usual convolution with a kernel of size (1, 1). Spatial information is not affected, but channels information is shared.\n",
    "\n",
    "![pointwise conv](images/pointwise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import DepthwiseConv2D\n",
    "\n",
    "separable_model = Sequential(name=\"Separable Model\")\n",
    "separable_model.add(DepthwiseConv2D(kernel_size=(3, 3), use_bias=False))\n",
    "separable_model.add(Conv2D(8, kernel_size=(1, 1), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice:\n",
    "\n",
    "- How many parameters does the Depthwise convolution has?\n",
    "- How many parameters does the Pointwise convolution has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separable_model.build((None, *input_shape))\n",
    "# separable_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignments\n",
    "\n",
    "- See the different models available on Keras: https://keras.io/applications/ What are their different architecture tricks?\n",
    "- Try to pick an architecture (like [MobileNet](https://arxiv.org/abs/1704.04861) or [Squeeze-and-Excitation network](https://arxiv.org/abs/1709.01507)), read their paper, implement it in Keras, and try to reach good performance on a small dataset like CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
