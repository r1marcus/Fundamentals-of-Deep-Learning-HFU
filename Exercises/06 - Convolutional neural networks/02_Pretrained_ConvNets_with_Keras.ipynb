{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a JPEG file as a numpy array\n",
    "\n",
    "Let's use [scikit-image](http://scikit-image.rg) to load the content of a JPEG file into a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "image = imread('laptop.jpeg')\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of the array are:\n",
    "- height\n",
    "- width\n",
    "- color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency reasons, the pixel intensities of each channel are stored as **8-bit unsigned integer** taking values in the **[0-255] range**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.min(), image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of a numpy array\n",
    "\n",
    "The size in bytes can be computed by multiplying the number of element by the size in byte of each element in the array.\n",
    "\n",
    "The size of one element depend of the data type.\n",
    "\n",
    "1 byte == 8 bits\n",
    "\n",
    "A byte in English is an octet in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.product(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "450 * 800 * 3 * (8 / 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check by asking numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image size: {:0.3} MB\".format(image.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing on the last dimension makes it possible to extract the 2D content of a specific color channel, for instance the red channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_channel = image[:, :, 0]\n",
    "red_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[:, :, 0], cmap=plt.cm.Reds_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Compute a grey-level version of the image with shape `(height, width)` by averaging the values across color channels using [image.mean](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html).\n",
    "\n",
    "- Plot the result with `plt.imshow` using a grey levels colormap.\n",
    "\n",
    "- Can the uint8 integer data type represent those average values? Check the data type used by numpy.\n",
    "\n",
    "- What is the size in (mega) bytes of this image? \n",
    "\n",
    "- What are the expected range of values for the new pixels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/grey_levels.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images, handling data types and dynamic ranges\n",
    "\n",
    "When dealing with an heterogeneous collection of image of various sizes, it is often necessary to resize the image to the same size. More specifically:\n",
    "\n",
    "- for **image classification**, most networks expect a specific **fixed input size**;\n",
    "\n",
    "- for **object detection** and instance segmentation, networks have more flexibility but the image should have **approximately the same size as the training set images**.\n",
    "\n",
    "Furthermore **large images can be much slower to process** than smaller images (the number of pixels varies quadratically with the height and width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "image = imread('laptop.jpeg')\n",
    "lowres_image = resize(image, (50, 50), mode='reflect', anti_aliasing=True)\n",
    "lowres_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lowres_image, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the pixels of the low resolution image are computed from by combining the values of the pixels in the high resolution image. The result is therefore represented as floating points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_image.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By conventions, both `skimage.transform.imresize` and `plt.imshow` assume that floating point values range from 0.0 to 1.0 when using floating points as opposed to 0 to 255 when using 8-bit integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_image.min(), lowres_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that keras on the other hand might expect images encoded with values in the `[0.0 - 255.0]` range irrespectively of the data type of the array. To avoid the implicit conversion to the `[0.0 - 1.0]` range we use the `preserve_range=True` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_large_range_image = resize(image, (50, 50), mode='reflect',\n",
    "                                  anti_aliasing=True, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_large_range_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_large_range_image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_large_range_image.min(), lowres_large_range_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** the behavior of `plt.imshow` depends on both the dtype and the dynamic range when displaying RGB images. In particular it does not work on RGB images with float64 values in the `[0.0 - 255.0]` range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lowres_large_range_image, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Suggest two possible ways to correctly display an RGB array with floating point values in the `[0.0 - 255.0]` range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/question_imshow_dtype_and_range.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Load a pre-trained ResNet50 pre-trained model using Keras Zoo\n",
    "- Build a headless model and compute representations of images\n",
    "- Explore the quality of representation with t-SNE\n",
    "- Retrain last layer on cat vs dog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "model = ResNet50(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of an image\n",
    "\n",
    "**Exercise**\n",
    "- Open an image, preprocess it and build a batch of 1 image\n",
    "- Use the model to classify this image\n",
    "- Decode the predictions using `decode_predictions` from Keras\n",
    "\n",
    "Notes:\n",
    "- Test your code with `\"images_resize/000007.jpg\"`\n",
    "- You may need `preprocess_input` for preprocessing the image. \n",
    "- The Keras resnet expects floating point images of size `(224, 224)` with a dynamic in `[0, 255]` before preprocessing. [skimage's resize](http://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize) has a `preserve_range` flag that you might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "path = \"laptop.jpeg\"\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/predict_image.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Taking snapshots from the webcam\n",
    "\n",
    "Let's use the [python API of OpenCV](pypi.python.org/pypi/opencv-python) to take pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def camera_grab(camera_id=0, fallback_filename=None):\n",
    "    camera = cv2.VideoCapture(camera_id)\n",
    "    try:\n",
    "        # take 10 consecutive snapshots to let the camera automatically tune\n",
    "        # itself and hope that the contrast and lighting of the last snapshot\n",
    "        # is good enough.\n",
    "        for i in range(10):\n",
    "            snapshot_ok, image = camera.read()\n",
    "        if snapshot_ok:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            print(\"WARNING: could not access camera\")\n",
    "            if fallback_filename:\n",
    "                image = imread(fallback_filename)\n",
    "    finally:\n",
    "        camera.release()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = camera_grab(camera_id=0, fallback_filename='laptop.jpeg')\n",
    "plt.imshow(image)\n",
    "print(\"dtype: {}, shape: {}, range: {}\".format(\n",
    "    image.dtype, image.shape, (image.min(), image.max())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Write a function named `classify` that takes a snapshot of the webcam and displays it along with the decoded predictions of model and their confidence level.\n",
    "\n",
    "- If you don't have access to a webcam take a picture with your mobile phone or a photo of your choice from the web, store it as a JPEG file on the disk instead and pass that file to the neural network to make the prediction.\n",
    "\n",
    "- Try to classify a photo of your face. Look at the confidence level. Can you explain the results?\n",
    "\n",
    "- Try to classify photos of common objects such as a book, a mobile phone, a cup... Try to center the objects and remove clutter to get confidence higher than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify():\n",
    "    # TODO: write me\n",
    "    pass\n",
    "\n",
    "    \n",
    "classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classify_webcam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the representations of images\n",
    "\n",
    "First let's make sure that we have access to a subset of image files from the PASCAL VOC dataset we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if not op.exists(\"images_resize\"):\n",
    "    print('Extracting image files...')\n",
    "    zf = ZipFile('images_pascalVOC.zip')\n",
    "    zf.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new model that maps the image input space to the output of the layer before the last layer of the pretrained Resnet 50 model. We call this new model the \"base model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = model.layers[0].input\n",
    "output = model.layers[-2].output\n",
    "base_model = Model(input, output)\n",
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model can transform any image into a flat, high dimensional, semantic feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = base_model.predict(img_batch)\n",
    "print(\"Shape of representation:\", representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing representations of all images can be time consuming.\n",
    "This is usually made by large batches on a GPU for massive performance gains.\n",
    "\n",
    "For the remaining part, we will use pre-computed representations saved in h5 format.\n",
    "\n",
    "For those interested, this is done using the `process_images.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "paths = [\"images_resize/\" + path\n",
    "         for path in sorted(os.listdir(\"images_resize/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('img_emb.h5', 'r') as h5f:\n",
    "    out_tensors = h5f['img_emb'][:]\n",
    "    \n",
    "out_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensors.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- What is the proportion of 0 values in this representation?\n",
    "- Can you find any negative values?\n",
    "- Why are there so many zero values?\n",
    "- Are the zero always located in the same dimensions for different input images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/representations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find a 2D representation of that high dimensional feature space using T-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "img_emb_tsne = TSNE(perplexity=30).fit_transform(out_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(img_emb_tsne[:, 0], img_emb_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add thumnails of the original images at their TSNE locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def imscatter(x, y, paths, ax=None, zoom=1, linewidth=0):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, p in zip(x, y, paths):\n",
    "        try:\n",
    "            im = imread(p)\n",
    "        except:\n",
    "            print(p)\n",
    "            continue\n",
    "        im = resize(im, (224, 224), preserve_range=False, mode='reflect')\n",
    "        im = OffsetImage(im, zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data',\n",
    "                            frameon=True, pad=0.1, \n",
    "                            bboxprops=dict(edgecolor='red',\n",
    "                                           linewidth=linewidth))\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "imscatter(img_emb_tsne[:, 0], img_emb_tsne[:, 1], paths, zoom=0.5, ax=ax)\n",
    "plt.savefig('tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Search: finding similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    plt.figure()\n",
    "    img = imread(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 57\n",
    "\n",
    "def most_similar(idx, top_n=5):\n",
    "    dists = np.linalg.norm(out_tensors - out_tensors[idx], axis = 1)\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    return sorted_dists[:top_n]\n",
    "\n",
    "sim = most_similar(idx)\n",
    "[display(paths[s]) for s in sim];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Classification from Nearest Neighbors?\n",
    "\n",
    "Using these representations, it may be possible to build a nearest neighbor classifier. However, the representations are learnt on ImageNet, which are centered images, when we input images from PascalVOC, more plausible inputs of a real world system.\n",
    "\n",
    "The next section explores this possibility by computing the histogram of similarities between one image and the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norms = np.linalg.norm(out_tensors, axis=1, keepdims=True)\n",
    "normed_out_tensors = out_tensors / out_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_idx = 208\n",
    "dists_to_item = np.linalg.norm(out_tensors - out_tensors[item_idx],\n",
    "                               axis=1)\n",
    "cos_to_item = np.dot(normed_out_tensors, normed_out_tensors[item_idx]) \n",
    "plt.hist(cos_to_item, bins=30)\n",
    "display(paths[item_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there is no clear separation of class boundaries visible in the histogram of similarities alone. We need some supervision to be able to classify images.\n",
    "\n",
    "With a labeled dataset, even with very little labels per class, one would be able to:\n",
    "- build a k-Nearest Neighbor model,\n",
    "- build a classification model such as a SVM.\n",
    "\n",
    "These approximate classifiers are useful in practice.\n",
    "See the `cat vs dog` home assignment with GPU for another example of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.where(cos_to_item > 0.5)\n",
    "print(items)\n",
    "[display(paths[s]) for s in items[0]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
